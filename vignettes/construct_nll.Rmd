---
title: "Choose your own optimizer - how to extract the function handlers"
author: "Phillip Brinck Vetter"
date: "2023-03-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ctsmrTMB)
```

In this document we show how to use the `construct_nll` method of a `ctsmrTMB` class object to construct and extract the objective function handlers for the objective function, its gradient and hessian (the latter is only available if the model does not contain any random effects parameters). These handlers can be parsed to an optimization algorithm of your own choice, or you can construct your own objective function which add regularization term on the fixed effects (although you will lose the automatic derivatives).

## Simulate from the Ornstein-Uhlenbeck process

We use the common Ornstein-Uhlenbeck process to showcase the use of `construct_nll`. 

$$ \mathrm{d}X_{t} = \theta (\mu - X_{t}) \, \mathrm{d}t \, + \sigma_{X} \, \mathrm{d}B_{t} $$
$$ Y_{t_{k}} = X_{t_{k}} + e_{t_{k}}, \qquad e_{t_{k}} \sim \mathcal{N}\left(0,\sigma_{Y}^{2}\right)  $$
We first create data by simulating the process

```{r}
# Simulate data using Euler Maruyama
set.seed(10)
theta=10; mu=1; sigma_x=1; sigma_y=1e-2
# 
dt.sim = 1e-3
t.sim = seq(0,1,by=dt.sim)
dw = rnorm(length(t.sim)-1,sd=sqrt(dt.sim))
x = 3
for(i in 1:(length(t.sim)-1)) {
  x[i+1] = x[i] + theta*(mu-x[i])*dt.sim + sigma_x*dw[i]
}

# Extract observations and add noise
dt.obs = 1e-2
t.obs = seq(0,1,by=dt.obs)
y = x[t.sim %in% t.obs] + sigma_y * rnorm(length(t.obs))

# Create data
.data = data.frame(
  t = t.obs,
  y = y
)
```


## Construct model object

We now construct the `ctsmrTMB` model object

```{r}
# Create model object
obj = ctsmrTMB$new()

# Set name of model (and the created .cpp file)
obj$set_modelname("ornstein_uhlenbeck")

# Add system equations
obj$add_systems(
  dx ~ theta * (mu-x) * dt + sigma_x*dw
)

# Add observation equations
obj$add_observations(
  y ~ x
)

# Set observation equation variances
obj$add_observation_variances(
  y ~ sigma_y^2
)

# Specify algebraic relations
obj$add_algebraics(
  theta ~ exp(logtheta),
  sigma_x ~ exp(logsigma_x),
  sigma_y ~ exp(logsigma_y)
)

# Specify parameter initial values and lower/upper bounds in estimation
obj$add_parameters(
  logtheta = log(c(init = 1, lower=1e-5, upper=50)),
  mu = c(init=1.5, lower=0, upper=5),
  logsigma_x = log(c(init= 1e-1, lower=1e-10, upper=10)),
  logsigma_y = log(c(init=1e-1, lower=1e-10, upper=10))
)

# Set initial state mean and covariance
obj$set_initial_state(x[1], 1e-1*diag(1))
```

## Estimation

We are in principle ready to call the `estimate` method to run the optimization scheme using the built-in optimization which uses `stats::nlminb` i.e.

```{r, echo=TRUE, eval=TRUE}
fit = obj$estimate(data=.data, 
                   method="ekf", 
                   ode.timestep=dt.sim,
                   silence=TRUE,
                   control=list(trace=0))
```

Inside the package we optimise the objective function with respect to the fixed effects using the construction function handlers from `TMB::MakeADFun` and parsing them to `stats::nlminb` i.e.

```{r}
# nll = TMB::MakeADFun(...)
# opt = nlminb(start=nll$par, objective=nll$fn, grad=nll$gr)
```

The `construct_nll` method allows you to retrieve the `nll` object defined above. You must supply arguments similarly to when calling `estimate`. The function handlers are contained in the `nll` list object and accessed as `nll$fn()`, `nll$gr` and `nll$he()` for the objective function, the gradient, and the hessian respectively. The initial value for the fixed effects parameters are stored in `nll$par`. 

The full list of arguments are as follows (see `?ctsmrTMB` for more information):
```{r}
nll = obj$construct_nll(data=.data,
                        ode.timestep=dt.sim,
                        method="ekf",
                        compile=FALSE,
                        silence=TRUE,
                        loss="quadratic",
                        loss_c=3)

# Initial parameters
nll$par

# Function handler
nll$fn(nll$par)

# Gradient handler
nll$gr(nll$par)

# Hessian handler
nll$he(nll$par)
```
We can now use these to optimise the function using e.g. `stats::optim` instead. You can extract the parameter bounds specified when calling `add_parameters()` method by using the `get_parameters` method (note that `nll$par` and `pars$initial` are identical).

```{r}
pars = obj$get_parameters()
print(pars)

opt = stats::optim(par=nll$par, 
                   fn=nll$fn, 
                   gr=nll$gr, 
                   method="L-BFGS-B", 
                   lower=pars$lower, 
                   upper=pars$upper)
```
Lets compare the results from using `stats::optim` with the extracted function handler versus the internal optimisation that uses `stats::nlminb` stored in `fit`:

```{r}
# Estimated parameters
data.frame(external=opt$par, internal=fit$par.fixed)

# Neg. Log-Likelihood
data.frame(external=opt$value, internal=fit$nll.value)

# Gradient components
data.frame(external=t(nll$gr(opt$par)), internal=t(nll$gr(fit$par.fixed)))
```


